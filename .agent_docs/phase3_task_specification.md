# Phase 3 Task Specification: ONNX Optimization

## üéØ Objective
Convert the PyTorch GPT-2 model to ONNX format and compare inference speed using ONNX Runtime.

## üì¶ Deliverables
1. ‚úÖ `src/optimize_onnx.py`: Script for ONNX conversion and inference.
2. ‚úÖ `models/gpt2.onnx`: The exported ONNX model.
3. ‚úÖ `results/onnx_results.json`: Latency metrics for ONNX.
4. ‚úÖ `results/charts/onnx_comparison.png`: Chart comparing PyTorch vs. ONNX.

## üìê Execution Steps

### Step 1: Implement `src/optimize_onnx.py`
**Goal**: Export model and run inference.

**Requirements**:
- Use `torch.onnx.export` to convert `gpt2`.
- **Handling KV-Cache**: 
  - *Option A (Recommended)*: Use `optimum.onnxruntime.ORTModelForCausalLM` to automatically handle export.
  - *Option B (Manual)*: Define dynamic axes for `input_ids` and `attention_mask`. For this MVP, you can start by exporting the **No-Cache** version first to ensure success.
- Implement `OnnxInference` class using `onnxruntime.InferenceSession`.

### Step 2: Run ONNX Inference
**Action**:
- Load the ONNX model.
- Run inference on the same prompt ("The future of artificial intelligence is").
- Measure latency (First Token & Per Token).

### Step 3: Compare & Visualize
**Action**:
- Load previous baseline results (`results/baseline_results.json`).
- Compare PyTorch (No-Cache) vs. ONNX (No-Cache).
- (Optional) Compare PyTorch (Cache) vs. ONNX (Cache) if export succeeded.
- Generate `results/charts/onnx_comparison.png` showing the speedup.

## üß™ Verification
- `models/gpt2.onnx` exists.
- `src/optimize_onnx.py` runs without error.
- Console output shows "ONNX Speedup: X.XXx".
